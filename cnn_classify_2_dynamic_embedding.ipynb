{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"cnn_classify_2_dynamic_embedding.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ja0CsB_SrMqI","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","from google.colab import drive\n","import os\n","\n","drive.mount('gdrive')\n","os.chdir('gdrive/My Drive/movie plot')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"erNf2HlWrSmU","colab_type":"code","colab":{}},"source":["class TextCNN():\n","  def __init__(\n","    self,sequence_length,num_classes,vocab_size,embedding_size,filter_sizes,num_filters,embedding_matrix):\n","\n","    self.input_x=tf.placeholder(tf.int32,[None,sequence_length],name=\"input_x\")\n","    self.input_y=tf.placeholder(tf.float32,[None,num_classes],name=\"input_y\")\n","    self.dropout_keep_prob=tf.placeholder(tf.float32,name=\"dropout_keep_prob\")\n","    \n","\n","    with tf.name_scope('embedding'):\n","        self.W=tf.Variable(embedding_matrix,name=\"W\",dtype=tf.float32)\n","        self.embedded=tf.nn.embedding_lookup(self.W,self.input_x)\n","        self.embedded_expanded=tf.expand_dims(self.embedded,-1)\n","    \n","    pooled_outputs=[]\n","    for i,filter_size in enumerate(filter_sizes):\n","        with tf.name_scope(\"conv_maxpool-%s\"%filter_size):\n","            filter_shape=[filter_size,embedding_size,1,num_filters]\n","            W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n","            b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n","\n","            conv=tf.nn.conv2d(\n","                self.embedded_expanded,W,strides=[1,1,1,1],padding=\"VALID\",name=\"conv\"\n","            )\n","            h=tf.nn.relu(tf.nn.bias_add(conv,b),name=\"relu\")\n","            pooled=tf.nn.max_pool(\n","                h,ksize=[1,sequence_length-filter_size+1,1,1],\n","                strides=[1,1,1,1],\n","                padding='VALID',\n","                name=\"pool\"\n","            )\n","            pooled_outputs.append(pooled)\n","\n","    num_filters_total=num_filters*len(filter_sizes)\n","    self.h_pool=tf.concat(pooled_outputs,3)\n","    self.h_pool_flat=tf.reshape(self.h_pool,[-1,num_filters_total])\n","\n","    with tf.name_scope(\"dropout\"):\n","      self.h_drop=tf.nn.dropout(self.h_pool_flat,self.dropout_keep_prob)\n","\n","    with tf.name_scope(\"output\"):\n","      W=tf.get_variable(\n","          \"W\",shape=[num_filters_total,num_classes],\n","          initializer=tf.contrib.layers.xavier_initializer()\n","      )\n","      b=tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n","      self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n","      self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n","\n","    with tf.name_scope(\"loss\"):\n","      losses=tf.nn.softmax_cross_entropy_with_logits(logits=self.scores,labels=self.input_y)\n","      self.loss=tf.reduce_mean(losses)\n","\n","    with tf.name_scope(\"accuracy\"):\n","      correct_predictions=tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n","      self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ChBDLKOn74gG","colab_type":"code","colab":{}},"source":["def train_step(sess,cnn,x_batch,y_batch,dropout_keep_prob,train_op,global_step):\n","  feed_dict={\n","      cnn.input_x:x_batch,\n","      cnn.input_y:y_batch,\n","      cnn.dropout_keep_prob:0.5\n","  }\n","  _, step,  loss, accuracy = sess.run(\n","        [train_op, global_step, cnn.loss, cnn.accuracy],\n","        feed_dict)\n","  print(\"step {}, loss {}, acc {}\".format(step, loss, accuracy))\n","\n","  return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHsMU9TA8V_s","colab_type":"code","colab":{}},"source":["def dev_step(sess,cnn,x_batch, y_batch, global_step):\n","    feed_dict = {\n","      cnn.input_x: x_batch,\n","      cnn.input_y: y_batch,\n","      cnn.dropout_keep_prob: 1.0\n","      }\n","    step,loss, accuracy = sess.run(\n","                      [global_step, cnn.loss, cnn.accuracy],\n","                      feed_dict)\n","    print(\"step {}, loss {:}, acc {:}\".format(step, loss, accuracy))\n","    return accuracy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKz1UC649MGt","colab_type":"code","colab":{}},"source":["def load_data():\n","  import pandas as pd\n","  from sklearn.preprocessing import OneHotEncoder\n","  enc=OneHotEncoder()\n","  X_train=np.load(\"dataset/X_train.npy\")\n","  y_train=np.load(\"dataset/y_train.npy\")\n","  y_train=enc.fit_transform(y_train.reshape(-1,1)).toarray()\n","  X_val=np.load(\"dataset/X_val.npy\")\n","  y_val=np.load(\"dataset/y_val.npy\")\n","  y_val=enc.fit_transform(y_val.reshape(-1,1)).toarray()\n","  X_test=np.load(\"dataset/X_test.npy\")\n","  y_test=np.load(\"dataset/y_test.npy\")\n","  y_test=enc.fit_transform(y_test.reshape(-1,1)).toarray()\n","\n","  return X_train.astype(int),y_train,X_val.astype(int),y_val,X_test.astype(int),y_test"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CMcAOhljgH2H","colab_type":"code","colab":{}},"source":["def batch_iter(data, batch_size, num_epochs):\n","    data = np.array(data)\n","    data_size = len(data)\n","    num_batches_per_epoch = int((len(data)-1)/batch_size) + 1\n","    for epoch in range(num_epochs):\n","        shuffle_indices = np.random.permutation(np.arange(data_size))\n","        shuffled_data = data[shuffle_indices]\n","        for batch_num in range(num_batches_per_epoch):\n","            start_index = batch_num * batch_size\n","            end_index = min((batch_num + 1) * batch_size, data_size)\n","            yield shuffled_data[start_index:end_index]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vKK21R0t5h8l","colab_type":"code","colab":{}},"source":["def train(graph, x_train,y_train,x_val,y_val,X_test,y_test,embedding_matrix,l2_clip_threshold,filter_sizes,num_filters,num_epochs):\n","  with graph.as_default():\n","    sess=tf.Session()\n","    with sess.as_default():\n","      cnn=TextCNN(\n","          sequence_length=x_train.shape[1],\n","          num_classes=y_train.shape[1],\n","          vocab_size=20001,\n","          embedding_size=300,\n","          filter_sizes=filter_sizes,\n","          num_filters=num_filters,\n","          embedding_matrix=embedding_matrix\n","      )\n","    \n","    global_step=tf.Variable(0,name=\"global_step\")\n","\n","    optimizer=tf.train.AdamOptimizer(0.001)\n","    grads_and_vars=optimizer.compute_gradients(cnn.loss)\n","    clipped_grads_and_vars = [(tf.clip_by_norm(gv[0],clip_norm=l2_clip_threshold), gv[1]) if gv[0] is not None else(gv[0],gv[1]) for gv in grads_and_vars]\n","    train_op=optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n","\n","\n","\n","    loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n","    acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n","    print(\"loss: \"+str(loss_summary)+\" acc: \"+str(acc_summary))\n","\n","\n","    checkpoint_dir = \"./cnn_checkpoint/cnn_dynamic_embedding_weights.ckpt\"\n","\n","    saver = tf.train.Saver(tf.global_variables(),checkpoint_dir, max_to_keep=max_num_checkpoints)\n","\n","    sess.run(tf.global_variables_initializer())\n","\n","    batches = batch_iter(\n","        list(zip(x_train, y_train)), batch_size, num_epochs)\n","\n","    train_acc=0\n","    dev_acc=0\n","\n","    for batch in batches:\n","        x_batch, y_batch = zip(*batch)\n","        train_acc = train_step(sess,cnn,x_batch, y_batch,dropout_keep_prob,train_op,global_step)\n","        current_step = tf.train.global_step(sess, global_step)\n","        if current_step % evaluate_every == 0:\n","            print(\"\\nEvaluation:\")\n","            dev_acc=dev_step(sess,cnn,x_val, y_val,global_step)\n","        if current_step % checkpoint_every == 0:\n","            path = saver.save(sess, checkpoint_dir)\n","            print(\"Saved model checkpoint to {}\\n\".format(path))\n","    \n","    print(\"***testing*****\\n\")\n","\n","    test_acc = dev_step(sess,cnn,X_test,y_test,global_step)\n","    return test_acc,train_acc,dev_acc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"W0-DFD2ohUQn","colab_type":"code","colab":{}},"source":["X_train,y_train,X_val,y_val,X_test,y_test=load_data()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kvJ6V9zUb3dV","colab_type":"code","colab":{}},"source":["embedding_dim=300\n","#filter_sizes=[3,4,5]\n","#num_filters=100\n","dropout_keep_prob=0.5\n","\n","\n","batch_size=50\n","#num_epochs=10\n","evaluate_every=100\n","checkpoint_every=100\n","max_num_checkpoints=1\n","l2_clip_threshold=3"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jziTjnYmiOFH","colab_type":"code","colab":{}},"source":["\n","embedding_matrix=np.load(\"dataset/embedding_matrix.npy\")\n","import csv\n","import time\n","best_test=0\n","for filter_sizes in [[3,4,5]]:\n","  for num_filters in [100]:\n","        graph=tf.Graph()\n","        test_acc,train_acc,dev_acc=train(graph,X_train,y_train,X_val,y_val,X_test,y_test,embedding_matrix,l2_clip_threshold,filter_sizes,num_filters,num_epochs)\n","        if test_acc>best_test:\n","          best_test=test_acc\n","          with open('dataset/dynamic_best.csv','a') as f:\n","            writer=csv.writer(f)\n","            t = time.localtime()\n","            current_time = time.strftime(\"%H:%M:%S\", t)\n","            writer.writerow([test_acc,train_acc,dev_acc,filter_sizes,num_filters,num_epochs,str(current_time)])\n","\n","with open('dataset/dynamic_best.csv','a') as f:\n","  writer=csv.writer(f)\n","  \n","  t = time.localtime()\n","  current_time = time.strftime(\"%H:%M:%S\", t)\n","  writer.writerow([str(current_time)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGMPHfGLwjSZ","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}