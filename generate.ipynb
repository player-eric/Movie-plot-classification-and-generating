{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"generate.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"exZoIOlBrgOH","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import numpy as np\n","from google.colab import drive\n","import os\n","\n","drive.mount('gdrive')\n","os.chdir('gdrive/My Drive/movie plot')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zH6zNjNkrxSl","colab_type":"code","colab":{}},"source":["import time\n","import numpy as np\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gN_GCncsyy03","colab_type":"code","colab":{}},"source":["import csv\n","\n","comedy_plots=[]\n","drama_plots=[]\n","thriller_plots=[]\n","horror_plots=[]\n","action_plots=[]\n","with open(\"dataset.csv\",'r') as f:\n","  reader = csv.reader(f)\n","  for row in reader:\n","    if row[1] =='comedy':\n","      comedy_plots.append(row[2])\n","    if row[1] =='drama':\n","      drama_plots.append(row[2])\n","    if row[1] =='thriller':\n","      thriller_plots.append(row[2])\n","    if row[1] =='horror':\n","      horror_plots.append(row[2])\n","    if row[1] =='action':\n","      action_plots.append(row[2])\n","\n","def printlen(a):\n","  for a_ in a:\n","    print(len(a_))\n","\n","printlen([comedy_plots,drama_plots,thriller_plots,horror_plots,action_plots])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8J7qLiu7KFJ","colab_type":"code","colab":{}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"N0fdswk72Szz","colab_type":"code","colab":{}},"source":["plots=[]\n","\n","import csv\n","with open(\"dataset.csv\",'r') as f:\n","  reader = csv.reader(f)\n","  for plot in reader:\n","    tok=Tokenizer()\n","    tok.fit_on_texts([str(plot[2])])\n","    seq=tok.texts_to_sequences([str(plot[2])])\n","    plots.append(len(seq[0]))\n","\n","import matplotlib.pyplot as plt\n","plots=np.array(plots)\n","\n","plt.hist(plots,bins='auto')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RdCnRyYt7rno","colab_type":"code","colab":{}},"source":["print(plots[2])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i-YHL3fkzWWc","colab_type":"code","colab":{}},"source":["comedy_plot=''\n","for plot in comedy_plots:\n","  comedy_plot+=plot\n","\n","vocab=set(comedy_plot)\n","print(vocab,len(vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hr7RrLjc1OTd","colab_type":"code","colab":{}},"source":["vocab_to_int={c:i for i,c in enumerate(vocab)}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FAyfXGW21bGT","colab_type":"code","colab":{}},"source":["int_to_vocab=dict(enumerate(vocab))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JPeBlAzd87Bm","colab_type":"code","colab":{}},"source":["encoded=np.array([vocab_to_int[c] for c in comedy_plot],dtype=np.int32)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r2FQabCF9MZv","colab_type":"code","colab":{}},"source":["print(comedy_plot[0:100])\n","print(encoded[0:100])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jVmvgwKfOmpd","colab_type":"code","colab":{}},"source":["print(len(encoded))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uHcPLBCJ1lbO","colab_type":"code","colab":{}},"source":["def get_batches(arr, n_seqs, n_steps):  \n","    batch_size = n_seqs * n_steps\n","    n_batches = int(len(arr) / batch_size)\n","\n","    arr = arr[:batch_size * n_batches]\n","    \n","    arr = arr.reshape((n_seqs, -1))\n","    \n","    for n in range(0, arr.shape[1], n_steps):\n","        # inputs\n","        x = arr[:, n:n+n_steps]\n","        # targets\n","        y = np.zeros_like(x)\n","        y[:, :-1], y[:, -1] = x[:, 1:], x[:, 0]\n","        yield x, y"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBDmbNVf6lbH","colab_type":"code","colab":{}},"source":["def build_inputs(num_seqs,num_steps):\n","  inputs=tf.placeholder(tf.int32,shape=(num_seqs,num_steps))\n","  targets=tf.placeholder(tf.int32,shape=(num_seqs,num_steps))\n","\n","  keep_prob=tf.placeholder(tf.float32,name='keep_prob')\n","\n","  return inputs,targets,keep_prob\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5lYFVO5N68Rr","colab_type":"code","colab":{}},"source":["def lstm_cell():\n","  lstm = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n","  return tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n","\n","def build_lstm(lstm_size, num_layers, batch_size, keep_prob):\n","\n","  cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n","  initial_state = cell.zero_state(batch_size, tf.float32)\n","  return cell, initial_state"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6tUr0sVC7lpd","colab_type":"code","colab":{}},"source":["def build_output(lstm_output, in_size, out_size):\n","    seq_output = tf.concat(lstm_output, axis=1) # tf.concat(concat_dim, values)\n","\n","    x = tf.reshape(seq_output, [-1, in_size])\n","    \n","    with tf.variable_scope('softmax'):\n","        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n","        softmax_b = tf.Variable(tf.zeros(out_size))\n","\n","    logits = tf.matmul(x, softmax_w) + softmax_b\n","    out = tf.nn.softmax(logits, name='predictions')\n","    \n","    return out, logits"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovwwrnpK8Nko","colab_type":"code","colab":{}},"source":["def build_loss(logits, targets, lstm_size, num_classes):\n","\n","    y_one_hot = tf.one_hot(targets, num_classes)\n","    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n","    \n","    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n","    loss = tf.reduce_mean(loss)\n","    \n","    return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCNyRHVs8PeX","colab_type":"code","colab":{}},"source":["def build_optimizer(loss, learning_rate, grad_clip):\n","\n","    tvars = tf.trainable_variables()\n","    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n","    train_op = tf.train.AdamOptimizer(learning_rate)\n","    optimizer = train_op.apply_gradients(zip(grads, tvars))\n","    \n","    return optimizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DfiSRH9g8R6u","colab_type":"code","colab":{}},"source":["class CharRNN:\n","    \n","    def __init__(self, num_classes, batch_size=64, num_steps=50, \n","                       lstm_size=128, num_layers=2, learning_rate=0.001, \n","                       grad_clip=5, sampling=False):\n","    \n","        if sampling == True:\n","            batch_size, num_steps = 1, 1\n","        else:\n","            batch_size, num_steps = batch_size, num_steps\n","\n","        tf.reset_default_graph()\n","\n","        self.inputs, self.targets, self.keep_prob = build_inputs(batch_size, num_steps)\n","\n","        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, self.keep_prob)\n","\n","        x_one_hot = tf.one_hot(self.inputs, num_classes)\n","\n","        outputs, state = tf.nn.dynamic_rnn(cell, x_one_hot, initial_state=self.initial_state)\n","        self.final_state = state\n","\n","        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n","\n","        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n","        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bw-VJOn8VhP","colab_type":"code","colab":{}},"source":["\n","batch_size = 100         # Sequences per batch\n","num_steps = 100          # Number of sequence steps per batch\n","lstm_size = 512         # Size of hidden layers in LSTMs\n","num_layers = 2          # Number of LSTM layers\n","learning_rate = 0.001    # Learning rate\n","keep_prob = 0.5         # Dropout keep probability"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8USntIYq8Yh1","colab_type":"code","colab":{}},"source":["epochs = 40\n","\n","save_every_n = 1000\n","\n","model = CharRNN(len(vocab), batch_size=batch_size, num_steps=num_steps,\n","                lstm_size=lstm_size, num_layers=num_layers, \n","                learning_rate=learning_rate)\n","\n","saver = tf.train.Saver(max_to_keep=10000)\n","with tf.Session() as sess:\n","\n","    sess.run(tf.global_variables_initializer())\n","    \n","    counter = 0\n","    for e in range(epochs):\n","        new_state = sess.run(model.initial_state)\n","        loss = 0\n","        for x, y in get_batches(encoded, batch_size, num_steps):\n","            counter += 1\n","            start = time.time()\n","            feed = {model.inputs: x,\n","                    model.targets: y,\n","                    model.keep_prob: keep_prob,\n","                    model.initial_state: new_state}\n","            batch_loss, new_state, _ = sess.run([model.loss, \n","                                                 model.final_state, \n","                                                 model.optimizer], \n","                                                 feed_dict=feed)\n","            end = time.time()\n","            if counter % 100 == 0:\n","                print('轮数: {}/{}... '.format(e+1, epochs),\n","                      '训练步数: {}... '.format(counter),\n","                      '训练误差: {:.4f}... '.format(batch_loss),\n","                      '{:.4f} sec/batch'.format((end-start)))\n","\n","            if (counter % save_every_n == 0):\n","                saver.save(sess, \"generate/checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))\n","    \n","    saver.save(sess, \"generate/checkpoints/i{}_l{}.ckpt\".format(counter, lstm_size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ko0zwPA9h68","colab_type":"code","colab":{}},"source":["\n","def pick_top_n(preds, vocab_size, top_n=3):\n","\n","    p = np.squeeze(preds)\n","    p[np.argsort(p)[:-top_n]] = 0\n","    p = p / np.sum(p)\n","    c = np.random.choice(vocab_size, 1, p=p)[0]\n","    return c"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"blv-lN9ZtBDc","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbWXDPqCZL5h","colab_type":"code","colab":{}},"source":["def sample(checkpoint, n_samples, lstm_size, vocab_size, prime=\"The \"):\n","\n","    samples = [c for c in prime]\n","    model = CharRNN(len(vocab), lstm_size=lstm_size, sampling=True)\n","    saver = tf.train.Saver()\n","    with tf.Session() as sess:\n","        saver.restore(sess, checkpoint)\n","        new_state = sess.run(model.initial_state)\n","        for c in prime:\n","            x = np.zeros((1, 1))\n","            x[0,0] = vocab_to_int[c]\n","            feed = {model.inputs: x,\n","                    model.keep_prob: 1.,\n","                    model.initial_state: new_state}\n","            preds, new_state = sess.run([model.prediction, model.final_state], \n","                                         feed_dict=feed)\n","\n","        c = pick_top_n(preds, len(vocab))\n","        samples.append(int_to_vocab[c])\n","        \n","        for i in range(n_samples):\n","            x[0,0] = c\n","            feed = {model.inputs: x,\n","                    model.keep_prob: 1.,\n","                    model.initial_state: new_state}\n","            preds, new_state = sess.run([model.prediction, model.final_state], \n","                                         feed_dict=feed)\n","\n","            c = pick_top_n(preds, len(vocab))\n","            samples.append(int_to_vocab[c])\n","        \n","    return ''.join(samples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8yxxHRzZTh6","colab_type":"code","colab":{}},"source":["\n","print(tf.train.latest_checkpoint('generate_comedy/checkpoints'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wG5Lgig-ZZTG","colab_type":"code","colab":{}},"source":["checkpoint = tf.train.latest_checkpoint('generate/checkpoints')\n","samp = sample(checkpoint, 500, lstm_size, len(vocab), prime=\"The \")\n","print(samp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jd2yPOenZlWc","colab_type":"code","colab":{}},"source":["for num in range(1000,8000,1000):\n","  print('generate_comedy/checkpoints/i{}_l512.ckpt'.format(num))\n","  checkpoint = 'generate_comedy/checkpoints/i{}_l512.ckpt'.format(num)\n","  samp = sample(checkpoint, 500, lstm_size, len(vocab), prime=\"Trump\")\n","  print(samp)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmrS9r5YbUUr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}